"""Configuration admin API endpoints for YAML cluster configuration management."""
from pathlib import Path
from typing import Any, Dict

import yaml
from fastapi import APIRouter, HTTPException

from ..config import get_cluster_config, reload_cluster_config

router = APIRouter()


def get_config_path() -> Path:
    """Get path to configuration file."""
    config = get_cluster_config()
    return config.config_path


@router.get("/config")
async def get_configuration():
    """Get current cluster configuration.

    Returns the complete YAML configuration including all clusters,
    node labels, account labels, and settings.
    """
    try:
        config = get_cluster_config()

        return {
            "clusters": config.config.get("clusters", {}),
            "settings": config.config.get("settings", {}),
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading configuration: {str(e)}")


@router.get("/config/{cluster_name}")
async def get_cluster_configuration(cluster_name: str):
    """Get configuration for a specific cluster.

    Args:
        cluster_name: Name of the cluster (e.g., "DAIC")

    Returns:
        Cluster configuration including labels and metadata
    """
    try:
        config = get_cluster_config()
        clusters = config.config.get("clusters", {})

        if cluster_name not in clusters:
            raise HTTPException(status_code=404, detail=f"Cluster {cluster_name} not found")

        return clusters[cluster_name]
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error loading cluster configuration: {str(e)}")


@router.post("/config/{cluster_name}/auto-generate")
async def auto_generate_cluster_configuration(cluster_name: str):
    """Auto-generate configuration for a cluster based on existing data.

    This endpoint analyzes the data for a cluster and automatically generates:
    - Node labels from unique node names
    - Account labels from unique account names
    - Partition labels from unique partition names

    Args:
        cluster_name: Name of the cluster

    Returns:
        Generated configuration with stats
    """
    try:
        from ..core.config import get_settings
        import pandas as pd

        settings = get_settings()
        data_path = Path(settings.data_path) / cluster_name / "data"

        if not data_path.exists():
            raise HTTPException(status_code=404, detail=f"No data directory found for cluster {cluster_name}")

        # Load all parquet files directly to get full column set (including NodeList)
        parquet_files = list(data_path.glob("*.parquet"))
        if not parquet_files:
            raise HTTPException(status_code=404, detail=f"No data files found for cluster {cluster_name}")

        # Load and concatenate all data files
        df_list = []
        for file in parquet_files:
            df_list.append(pd.read_parquet(file))
        df = pd.concat(df_list, ignore_index=True)

        if df.empty:
            raise HTTPException(status_code=404, detail=f"No data found for cluster {cluster_name}")

        def expand_slurm_nodelist(nodelist_str: str) -> set[str]:
            """Expand SLURM compact node list notation.

            Examples:
                'node[01-03]' -> {'node01', 'node02', 'node03'}
                'gpu[1,3,5]' -> {'gpu1', 'gpu3', 'gpu5'}
                'node01,node02' -> {'node01', 'node02'}
            """
            import re
            nodes = set()

            # Split by comma first
            parts = [p.strip() for p in nodelist_str.split(',')]

            for part in parts:
                if not part:
                    continue

                # Check for bracket notation: prefix[range] or prefix[list]
                match = re.match(r'^([a-zA-Z_-]+)\[([^\]]+)\]$', part)
                if match:
                    prefix = match.group(1)
                    bracket_content = match.group(2)

                    # Split by comma to handle mixed notation like gpu[4,5-6,9]
                    items = [item.strip() for item in bracket_content.split(',')]

                    for item in items:
                        if not item:
                            continue

                        # Check if this item is a range (e.g., "5-6")
                        if '-' in item:
                            parts = item.split('-')
                            if len(parts) == 2:
                                try:
                                    start_str, end_str = parts
                                    # Detect if it's zero-padded
                                    padding = len(start_str) if start_str and start_str[0] == '0' else 0
                                    start = int(start_str)
                                    end = int(end_str)
                                    for i in range(start, end + 1):
                                        if padding:
                                            nodes.add(f"{prefix}{i:0{padding}d}")
                                        else:
                                            nodes.add(f"{prefix}{i}")
                                except (ValueError, IndexError):
                                    # Not a valid range, add as-is
                                    nodes.add(f"{prefix}{item}")
                            else:
                                # Multiple dashes or invalid format, add as-is
                                nodes.add(f"{prefix}{item}")
                        else:
                            # Single item, not a range
                            nodes.add(f"{prefix}{item}")
                else:
                    # No bracket notation, add as-is
                    nodes.add(part)

            return nodes

        # Generate node labels from unique node names
        node_labels = {}
        if "NodeList" in df.columns:
            # NodeList can be strings (possibly comma-separated) or arrays
            all_nodes = set()
            for nodelist in df["NodeList"].dropna():
                if isinstance(nodelist, str):
                    # Expand SLURM compact notation
                    expanded = expand_slurm_nodelist(nodelist)
                    all_nodes.update(expanded)
                elif isinstance(nodelist, (list, tuple)):
                    # It's a list/tuple of nodes
                    for node in nodelist:
                        if node and isinstance(node, str):
                            all_nodes.add(node)
                else:
                    # Handle numpy arrays or other iterables (but not strings!)
                    try:
                        import numpy as np
                        if isinstance(nodelist, np.ndarray):
                            for node in nodelist:
                                if node and isinstance(node, str):
                                    all_nodes.add(node)
                    except (TypeError, AttributeError, ImportError):
                        pass

            for node in sorted(all_nodes):
                # Try to detect node type from name
                node_type = "cpu"
                if "gpu" in node.lower():
                    node_type = "gpu"
                elif "login" in node.lower():
                    node_type = "login"
                elif "storage" in node.lower() or "nas" in node.lower():
                    node_type = "storage"

                node_labels[node] = {
                    "synonyms": [],
                    "type": node_type,
                    "description": f"{node_type.upper()} Node {node}",
                }

        # Generate account labels
        account_labels = {}
        if "Account" in df.columns:
            unique_accounts = df["Account"].dropna().unique()
            for account in sorted(unique_accounts):
                if account and isinstance(account, str):
                    account_labels[account] = {
                        "display_name": account,
                        "short_name": account.split("-")[-1].upper() if "-" in account else account,
                    }

        # Generate partition labels
        partition_labels = {}
        if "Partition" in df.columns:
            # Split comma-separated partitions to get individual partitions
            all_partitions = set()
            for partition_str in df["Partition"].dropna().unique():
                if partition_str and isinstance(partition_str, str):
                    # Split by comma and strip whitespace
                    partitions = [p.strip() for p in partition_str.split(',') if p.strip()]
                    all_partitions.update(partitions)

            for partition in sorted(all_partitions):
                partition_labels[partition] = {
                    "display_name": f"{partition.capitalize()} Partition",
                    "description": f"{partition} partition",
                }

        # Create cluster configuration
        cluster_config = {
            "display_name": f"{cluster_name} Cluster",
            "description": f"Auto-generated configuration for {cluster_name}",
            "metadata": {
                "location": "Unknown",
                "owner": "Unknown",
                "contact": "unknown@example.com",
            },
            "node_labels": node_labels,
            "account_labels": account_labels,
            "partition_labels": partition_labels,
        }

        # Load current configuration
        config_path = get_config_path()
        if config_path.exists():
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f) or {}
        else:
            config_data = {"clusters": {}, "settings": {}}

        # Ensure clusters key exists
        if "clusters" not in config_data:
            config_data["clusters"] = {}

        # Check if cluster already exists
        if cluster_name in config_data["clusters"]:
            # Merge with existing configuration (don't overwrite existing labels)
            existing = config_data["clusters"][cluster_name]

            # Merge node labels (add new ones, keep existing)
            if "node_labels" in existing:
                for node, config in node_labels.items():
                    if node not in existing["node_labels"]:
                        existing["node_labels"][node] = config
            else:
                existing["node_labels"] = node_labels

            # Merge account labels
            if "account_labels" in existing:
                for account, config in account_labels.items():
                    if account not in existing["account_labels"]:
                        existing["account_labels"][account] = config
            else:
                existing["account_labels"] = account_labels

            # Merge partition labels
            if "partition_labels" in existing:
                for partition, config in partition_labels.items():
                    if partition not in existing["partition_labels"]:
                        existing["partition_labels"][partition] = config
            else:
                existing["partition_labels"] = partition_labels

            cluster_config = existing
        else:
            # Add new cluster configuration
            config_data["clusters"][cluster_name] = cluster_config

        # Write back to file using atomic write (temp file + rename)
        import tempfile
        import os

        # Create temp file in the same directory to ensure same filesystem
        config_dir = config_path.parent
        with tempfile.NamedTemporaryFile(mode='w', dir=config_dir, delete=False, suffix='.yaml') as f:
            temp_path = f.name
            yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)

        # Atomically replace the original file
        os.replace(temp_path, config_path)

        # Reload configuration
        reload_cluster_config()

        return {
            "status": "success",
            "message": f"Configuration auto-generated for cluster {cluster_name}",
            "configuration": cluster_config,
            "stats": {
                "nodes": len(node_labels),
                "accounts": len(account_labels),
                "partitions": len(partition_labels),
            },
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500, detail=f"Error auto-generating configuration: {str(e)}"
        )


@router.post("/config/reload")
async def reload_configuration():
    """Reload configuration from file.

    Useful after manual edits to the YAML file.

    Returns:
        Success message with loaded clusters
    """
    try:
        config = reload_cluster_config()
        clusters = config.get_all_clusters()

        return {
            "status": "success",
            "message": "Configuration reloaded successfully",
            "clusters": clusters,
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reloading configuration: {str(e)}")


@router.put("/config")
async def update_configuration(config_data: Dict[str, Any]):
    """Update the entire cluster configuration.

    This endpoint allows you to update the complete YAML configuration.
    The configuration will be validated and written to the file.

    Args:
        config_data: Complete configuration object with clusters and settings

    Returns:
        Success message
    """
    try:
        # Validate that we have the required structure
        if "clusters" not in config_data:
            raise HTTPException(status_code=400, detail="Configuration must include 'clusters' key")

        # Get config path
        config_path = get_config_path()

        # Create backup of existing config
        if config_path.exists():
            backup_path = config_path.with_suffix(".yaml.backup")
            import shutil
            shutil.copy2(config_path, backup_path)

        # Write new configuration
        with open(config_path, "w") as f:
            yaml.dump(config_data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)

        # Reload configuration to validate
        reload_cluster_config()

        return {
            "status": "success",
            "message": "Configuration updated successfully",
        }
    except HTTPException:
        raise
    except Exception as e:
        # If there was an error, try to restore from backup
        config_path = get_config_path()
        backup_path = config_path.with_suffix(".yaml.backup")
        if backup_path.exists():
            import shutil
            shutil.copy2(backup_path, config_path)
            reload_cluster_config()

        raise HTTPException(status_code=500, detail=f"Error updating configuration: {str(e)}")


@router.post("/generate-demo-cluster")
async def generate_demo_cluster():
    """Generate a demo cluster with 2 years of synthetic data.

    Creates a DemoCluster with:
    - 2 years of data (2023-2024)
    - 100 users (power users, regular users, students)
    - Seasonal patterns (more active in spring and fall)
    - Simulated outages with longer waiting times
    - 30 nodes (15 GPU, 15 CPU)
    - 3 partitions (general, cpu, gpu)
    - Realistic job patterns and resource usage

    Returns:
        Status and statistics about generated data
    """
    try:
        import sys
        import subprocess
        from pathlib import Path
        from datetime import datetime, date

        # Import the generator
        sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "scripts"))
        from generate_test_cluster_data import SyntheticClusterDataGenerator

        cluster_name = "DemoCluster"

        # Get data directory
        from ..core.config import get_settings
        settings = get_settings()
        output_dir = Path(settings.data_path) / cluster_name / "data"

        # Check if demo cluster already exists
        if output_dir.exists() and list(output_dir.glob("*.parquet")):
            raise HTTPException(
                status_code=400,
                detail=f"Demo cluster already exists at {output_dir}. Delete it first to regenerate."
            )

        # Define outage periods (2-3 outages during the 2-year period)
        outages = [
            (date(2023, 6, 15), date(2023, 6, 18)),   # 3-day summer outage
            (date(2023, 11, 20), date(2023, 11, 22)), # 2-day fall outage
            (date(2024, 4, 10), date(2024, 4, 13))    # 3-day spring outage
        ]

        # Create generator with demo configuration
        generator = SyntheticClusterDataGenerator(
            cluster_name=cluster_name,
            seed=42,
            num_users=100,
            simple_partitions=True
        )

        # Generate dataset
        start_date = datetime(2023, 1, 1)
        end_date = datetime(2024, 12, 31)
        jobs_per_day = 150  # Average jobs per day

        df = generator.generate_dataset(
            start_date=start_date,
            end_date=end_date,
            jobs_per_day=jobs_per_day,
            seasonal_pattern=True,
            outages=outages
        )

        # Save data
        generator.save_weekly_data(df, output_dir)

        # Auto-generate configuration
        from pathlib import Path
        import pandas as pd

        config_path = get_config_path()
        if config_path.exists():
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f) or {}
        else:
            config_data = {"clusters": {}, "settings": {}}

        if "clusters" not in config_data:
            config_data["clusters"] = {}

        # Create cluster configuration
        cluster_config = {
            "display_name": "Demo Cluster",
            "description": "Synthetic demo cluster with 2 years of realistic job data",
            "metadata": {
                "location": "Demo Environment",
                "owner": "Demo",
                "contact": "demo@example.com",
            },
            "node_labels": {},
            "account_labels": {},
            "partition_labels": {
                "general": {
                    "display_name": "General Partition",
                    "description": "General purpose partition"
                },
                "cpu": {
                    "display_name": "CPU Partition",
                    "description": "CPU-only partition"
                },
                "gpu": {
                    "display_name": "GPU Partition",
                    "description": "GPU partition"
                }
            }
        }

        # Extract node and account info from generated data
        all_nodes = set()
        for nodes_str in df["NodeList"].dropna().unique():
            if isinstance(nodes_str, str):
                all_nodes.update(nodes_str.split(","))

        for node in sorted(all_nodes):
            node_type = "gpu" if "gpu" in node.lower() else "cpu"
            cluster_config["node_labels"][node] = {
                "synonyms": [],
                "type": node_type,
                "description": f"{node_type.upper()} Node {node}"
            }

        for account in sorted(df["Account"].dropna().unique()):
            cluster_config["account_labels"][account] = {
                "display_name": account,
                "short_name": account.split("-")[-1].upper() if "-" in account else account
            }

        config_data["clusters"][cluster_name] = cluster_config

        # Write configuration atomically
        import tempfile
        import os
        config_dir = config_path.parent
        with tempfile.NamedTemporaryFile(mode='w', dir=config_dir, delete=False, suffix='.yaml') as f:
            temp_path = f.name
            yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)
        os.replace(temp_path, config_path)

        # Reload configuration and datastore
        reload_cluster_config()

        # Trigger datastore reload
        from ..datastore_singleton import get_datastore
        datastore = get_datastore()
        datastore.check_for_updates()

        # Create physical cluster entry in database
        from ..database.cluster_db import get_cluster_db
        import secrets

        cluster_db = get_cluster_db()

        # Check if demo cluster already exists in database
        existing_clusters = cluster_db.get_all_clusters()
        demo_exists = any(c["name"] == cluster_name for c in existing_clusters)

        if not demo_exists:
            # Generate API key
            api_key = secrets.token_urlsafe(32)

            # Create cluster in database
            cluster_db.create_cluster(
                name=cluster_name,
                description="Synthetic demo cluster with 2 years of realistic job data (2023-2024)",
                contact_email="demo@example.com",
                location="Demo Environment",
                api_key=api_key
            )

        return {
            "status": "success",
            "message": f"Demo cluster generated successfully",
            "cluster_name": cluster_name,
            "stats": {
                "total_jobs": len(df),
                "date_range": f"{df['Submit'].min()} to {df['Submit'].max()}",
                "users": len(df["User"].unique()),
                "accounts": len(df["Account"].unique()),
                "partitions": len(df["Partition"].unique()),
                "nodes": len(all_nodes),
                "total_cpu_hours": float(df["CPU-hours"].sum()),
                "total_gpu_hours": float(df["GPU-hours"].sum()),
                "outages": len(outages)
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        raise HTTPException(
            status_code=500,
            detail=f"Error generating demo cluster: {str(e)}\n{traceback.format_exc()}"
        )


@router.delete("/config/{cluster_name}/cleanup")
async def cleanup_demo_cluster(cluster_name: str):
    """Delete a demo cluster's data and configuration.

    This removes:
    - Data directory
    - YAML configuration entry
    - Database entry (if exists)

    Args:
        cluster_name: Name of the cluster to delete

    Returns:
        Success message
    """
    try:
        from pathlib import Path
        from ..core.config import get_settings
        import shutil

        settings = get_settings()

        # Delete data directory
        data_dir = Path(settings.data_path) / cluster_name
        if data_dir.exists():
            shutil.rmtree(data_dir)

        # Remove from YAML configuration
        config_path = get_config_path()
        if config_path.exists():
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f) or {}

            if "clusters" in config_data and cluster_name in config_data["clusters"]:
                del config_data["clusters"][cluster_name]

                # Write back
                import tempfile
                import os
                config_dir = config_path.parent
                with tempfile.NamedTemporaryFile(mode='w', dir=config_dir, delete=False, suffix='.yaml') as f:
                    temp_path = f.name
                    yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)
                os.replace(temp_path, config_path)

        # Remove from database if exists
        from ..database.cluster_db import get_cluster_db
        cluster_db = get_cluster_db()
        existing_clusters = cluster_db.get_all_clusters()
        for cluster in existing_clusters:
            if cluster["name"] == cluster_name:
                cluster_db.delete_cluster(cluster["id"])
                break

        # Reload configuration
        reload_cluster_config()

        # Trigger datastore reload
        from ..datastore_singleton import get_datastore
        datastore = get_datastore()
        datastore.check_for_updates()

        return {
            "status": "success",
            "message": f"Cluster {cluster_name} cleaned up successfully"
        }
    except Exception as e:
        import traceback
        raise HTTPException(
            status_code=500,
            detail=f"Error cleaning up cluster: {str(e)}\n{traceback.format_exc()}"
        )


@router.put("/config/{cluster_name}")
async def update_cluster_configuration(cluster_name: str, cluster_data: Dict[str, Any]):
    """Update configuration for a specific cluster.

    This endpoint allows you to update the configuration for a single cluster.
    More convenient than updating the entire configuration file.

    Args:
        cluster_name: Name of the cluster (e.g., "DAIC")
        cluster_data: Cluster configuration object

    Returns:
        Success message
    """
    try:
        # Get config path
        config_path = get_config_path()

        # Load current configuration
        if config_path.exists():
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f) or {}
        else:
            config_data = {"clusters": {}, "settings": {}}

        # Ensure clusters key exists
        if "clusters" not in config_data:
            config_data["clusters"] = {}

        # Create backup
        backup_path = config_path.with_suffix(".yaml.backup")
        if config_path.exists():
            import shutil
            shutil.copy2(config_path, backup_path)

        # Update the specific cluster
        config_data["clusters"][cluster_name] = cluster_data

        # Write updated configuration
        with open(config_path, "w") as f:
            yaml.dump(config_data, f, default_flow_style=False, sort_keys=False, allow_unicode=True)

        # Reload configuration to validate
        reload_cluster_config()

        return {
            "status": "success",
            "message": f"Configuration updated successfully for cluster {cluster_name}",
        }
    except Exception as e:
        # If there was an error, try to restore from backup
        config_path = get_config_path()
        backup_path = config_path.with_suffix(".yaml.backup")
        if backup_path.exists():
            import shutil
            shutil.copy2(backup_path, config_path)
            reload_cluster_config()

        raise HTTPException(status_code=500, detail=f"Error updating cluster configuration: {str(e)}")
