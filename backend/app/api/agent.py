"""API endpoints for agent data uploads."""

import logging
import os
from pathlib import Path
from typing import Annotated

import yaml
from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile, status

from ..core.agent_auth import verify_agent_api_key
from ..core.config import get_settings

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/agent", tags=["agent"])


@router.post("/exchange-deploy-key")
async def exchange_deploy_key(deploy_key: str = Form(...)) -> dict:
    """Exchange a one-time deploy key for the actual API key.

    This endpoint allows agents to use a deploy key on first run to fetch
    the actual API key. The deploy key is invalidated after one use.

    Args:
        deploy_key: One-time deployment key generated by admin

    Returns:
        Cluster information with API key

    Example:
        ```bash
        curl -X POST "http://localhost:8100/api/agent/exchange-deploy-key" \\
          -F "deploy_key=deploy_abc123..."
        ```
    """
    from ..db.clusters import get_cluster_db

    cluster_db = get_cluster_db()
    result = cluster_db.exchange_deploy_key(deploy_key)

    if not result:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid or already used deploy key",
        )

    logger.info(f"Deploy key exchanged for cluster {result['cluster_name']}")

    return {
        "status": "success",
        "message": "Deploy key exchanged successfully. Save the API key securely.",
        "cluster_name": result["cluster_name"],
        "api_key": result["api_key"],
    }


@router.post("/upload", status_code=status.HTTP_201_CREATED)
async def upload_data(
    file: Annotated[UploadFile, File(description="Parquet file with SLURM data")],
    cluster_name: str = Depends(verify_agent_api_key),
) -> dict:
    """Upload SLURM data from agent.

    This endpoint allows agents to upload collected SLURM data via API instead of
    requiring shared filesystem access. The cluster is automatically determined from
    the API key.

    Args:
        file: Parquet file containing SLURM job data
        cluster_name: Cluster name from API key verification (injected automatically)

    Returns:
        Success message with file location

    Example:
        ```bash
        curl -X POST "http://localhost:8100/api/agent/upload" \\
          -H "X-API-Key: your-api-key-here" \\
          -F "file=@2024-W45.parquet"
        ```
    """
    settings = get_settings()

    # Validate file extension
    if not file.filename or not file.filename.endswith(".parquet"):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="File must be a .parquet file",
        )

    # Create directory structure: DATA_PATH/cluster_name/data/
    cluster_dir = Path(settings.data_path) / cluster_name / "data"
    cluster_dir.mkdir(parents=True, exist_ok=True)

    # Save file
    file_path = cluster_dir / file.filename
    try:
        contents = await file.read()
        with open(file_path, "wb") as f:
            f.write(contents)

        logger.info(
            f"Agent uploaded data: cluster={cluster_name}, "
            f"file={file.filename}, size={len(contents)} bytes"
        )

        # Trigger immediate data reload and cache invalidation
        try:
            from ..datastore_singleton import get_datastore
            from .charts import clear_chart_cache

            datastore = get_datastore()
            updated = datastore.check_for_updates()
            clear_chart_cache()

            if updated:
                logger.info(f"Data reloaded after upload for cluster={cluster_name}")
        except Exception as e:
            logger.warning(f"Failed to trigger immediate reload: {e}")

        return {
            "status": "success",
            "message": f"File uploaded successfully: {file.filename}",
            "cluster": cluster_name,
            "file": file.filename,
            "size": len(contents),
            "path": str(file_path.relative_to(settings.data_path)),
        }

    except Exception as e:
        logger.error(f"Failed to save uploaded file: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save file: {str(e)}",
        ) from e


@router.get("/health")
async def health_check(api_key: str = Depends(verify_agent_api_key)) -> dict:
    """Health check endpoint for agents.

    Verifies that the agent can authenticate and the API is operational.

    Args:
        api_key: Verified API key from header

    Returns:
        Health status
    """
    settings = get_settings()
    data_path = Path(settings.data_path)

    return {
        "status": "healthy",
        "data_path": str(data_path),
        "data_path_exists": data_path.exists(),
        "data_path_writable": data_path.exists() and os.access(data_path, os.W_OK),
    }


@router.get("/list-files")
async def list_uploaded_files(
    cluster_name: str = Depends(verify_agent_api_key),
) -> dict:
    """List uploaded data files for authenticated cluster.

    This endpoint allows agents to check which files have already been uploaded
    to the server, enabling local file cleanup without re-uploading.
    The cluster is automatically determined from the API key.

    Args:
        cluster_name: Cluster name from API key verification (injected automatically)

    Returns:
        Dictionary with list of uploaded filenames

    Example:
        ```bash
        curl -X GET "http://localhost:8100/api/agent/list-files" \\
          -H "X-API-Key: your-api-key-here"
        ```
    """
    settings = get_settings()

    # Check cluster directory
    cluster_dir = Path(settings.data_path) / cluster_name / "data"

    if not cluster_dir.exists():
        return {
            "cluster": cluster_name,
            "files": [],
            "count": 0,
            "message": "No data directory found for this cluster",
        }

    try:
        # List all parquet files
        parquet_files = list(cluster_dir.glob("*.parquet"))
        filenames = [f.name for f in parquet_files]

        logger.info(f"Agent requested file list for {cluster_name}: {len(filenames)} files")

        return {
            "cluster": cluster_name,
            "files": sorted(filenames),
            "count": len(filenames),
        }

    except Exception as e:
        logger.error(f"Failed to list files for {cluster_name}: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list files: {str(e)}",
        ) from e


@router.post("/upload-config", status_code=status.HTTP_201_CREATED)
async def upload_cluster_config(
    file: Annotated[UploadFile, File(description="Cluster configuration YAML file")],
    cluster_name: str = Depends(verify_agent_api_key),
) -> dict:
    """Upload cluster configuration YAML.

    This endpoint allows agents to upload cluster configuration metadata
    including node labels, account mappings, and partition information.
    The cluster is automatically determined from the API key.

    Args:
        file: YAML file with cluster configuration
        cluster_name: Cluster name from API key verification (injected automatically)

    Returns:
        Success message with config location

    Example:
        ```bash
        curl -X POST "http://localhost:8100/api/agent/upload-config" \\
          -H "X-API-Key: your-api-key-here" \\
          -F "file=@cluster-config.yaml"
        ```
    """
    # Validate file extension
    if not file.filename or not (file.filename.endswith(".yaml") or file.filename.endswith(".yml")):
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="File must be a .yaml or .yml file",
        )

    # Read and validate YAML
    try:
        contents = await file.read()
        config_data = yaml.safe_load(contents)

        if not isinstance(config_data, dict):
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="Invalid YAML structure. Must be a dictionary.",
            )

    except yaml.YAMLError as e:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Invalid YAML syntax: {str(e)}",
        ) from e

    # Save configuration to config directory
    # Use project root /config directory
    config_dir = Path(__file__).parent.parent.parent / "config"
    config_dir.mkdir(exist_ok=True)

    # For now, save as cluster-specific config
    # Later we can merge into clusters.yaml
    config_file = config_dir / f"{cluster_name}.yaml"

    try:
        with open(config_file, "w") as f:
            yaml.dump(config_data, f, default_flow_style=False, sort_keys=False)

        logger.info(
            f"Agent uploaded cluster config: cluster={cluster_name}, "
            f"file={file.filename}, size={len(contents)} bytes"
        )

        return {
            "status": "success",
            "message": f"Cluster configuration uploaded successfully: {file.filename}",
            "cluster": cluster_name,
            "file": file.filename,
            "size": len(contents),
            "saved_to": str(config_file),
        }

    except Exception as e:
        logger.error(f"Failed to save cluster config: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save configuration: {str(e)}",
        ) from e
